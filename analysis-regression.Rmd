---
title: "Regression"
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      error = TRUE,
                      comment = "")
```

# Preamble

## Install Libraries

```{r}
#install.packages("remotes")
#remotes::install_git("https://research-git.uiowa.edu/PetersenLab/petersenlab.git")
```

## Load Libraries

```{r, message = FALSE, warning = FALSE}
library("psych")
library("rms")
library("robustbase")
library("brms")
```

# Import data

```{r}
mydata <- read.csv("https://osf.io/8syp5/download")
```

# Data preparation

```{r}
mydata$countVariable <- as.integer(mydata$bpi_antisocialT2Sum)
mydata$orderedVariable <- factor(mydata$countVariable, ordered = TRUE)

mydata$female <- NA
mydata$female[which(mydata$sex == "male")] <- 0
mydata$female[which(mydata$sex == "female")] <- 1
```

# Linear Regression

## Linear regression model

```{r}
multipleRegressionModel <- lm(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                              data = mydata,
                              na.action = na.exclude)
multipleRegressionModelNoMissing <- lm(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                       data = mydata,
                                       na.action = na.omit)

summary(multipleRegressionModel)
confint(multipleRegressionModel)
```

## Linear regression model on correlation/covariance matrix (for pairwise deletion)

Also see here: https://stats.stackexchange.com/questions/158366/fit-multiple-regression-model-with-pairwise-deletion-or-on-a-correlation-covari/173002#173002

```{r}
multipleRegressionModelPairwise <- setCor(y = "bpi_antisocialT2Sum",
                                          x = c("bpi_antisocialT1Sum","bpi_anxiousDepressedSum"),
                                          data = cov(mydata[,c("bpi_antisocialT2Sum","bpi_antisocialT1Sum","bpi_anxiousDepressedSum")], use = "pairwise.complete.obs"),
                                          n.obs = nrow(mydata))

summary(multipleRegressionModelPairwise)
multipleRegressionModelPairwise[c("coefficients","se","Probability","R2","shrunkenR2")]
```

## Linear regression model with robust covariance matrix (rms)

```{r}
rmsMultipleRegressionModel <- robcov(ols(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                         data = mydata,
                                         x = TRUE,
                                         y = TRUE))

rmsMultipleRegressionModel
confint(rmsMultipleRegressionModel)
```

## Robust linear regression (MM-type iteratively reweighted least squares regression)

```{r}
robustLinearRegression <- lmrob(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                data = mydata,
                                na.action = na.exclude)

summary(robustLinearRegression)
confint(robustLinearRegression)
```

## Least trimmed squares regression (for removing outliers)

```{r}
ltsRegression <- ltsReg(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                        data = mydata,
                        na.action = na.exclude)

summary(ltsRegression)
```

## Bayesian linear regression

```{r}
bayesianRegularizedRegression <- brm(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                     data = mydata,
                                     chains = 4,
                                     iter = 2000,
                                     seed = 52242)

summary(bayesianRegularizedRegression)
```

# Generalized Linear Regression

## Generalized regression model

In this example, we predict a count variable that has a poisson distribution.
We could change the distribution.

```{r}
generalizedRegressionModel <- glm(countVariable ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                  data = mydata,
                                  family = "poisson",
                                  na.action = na.exclude)

summary(generalizedRegressionModel)
confint(generalizedRegressionModel)
```

## Generalized regression model (rms)

In this example, we predict a count variable that has a poisson distribution.
We could change the distribution.

```{r}
rmsGeneralizedRegressionModel <- Glm(countVariable ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                     data = mydata,
                                     x = TRUE,
                                     y = TRUE,
                                     family = "poisson")

rmsGeneralizedRegressionModel
confint(rmsGeneralizedRegressionModel)
```

## Bayesian generalized linear model

In this example, we predict a count variable that has a poisson distribution.
We could change the distribution.
For example, we could use Gamma regression, `family = Gamma`, when the response variable is continuous and positive, and the coefficient of variation--rather than the variance--is constant.

```{r}
bayesianGeneralizedLinearRegression <- brm(countVariable ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                           data = mydata,
                                           family = poisson,
                                           chains = 4,
                                           seed = 52242,
                                           iter = 2000)

summary(bayesianGeneralizedLinearRegression)
```

## Robust generalized regression

```{r}
robustGeneralizedRegression <- glmrob(countVariable ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                      data = mydata,
                                      family = "poisson",
                                      na.action = na.exclude)

summary(robustGeneralizedRegression)
confint(robustGeneralizedRegression)
```

## Ordinal regression model (rms)

```{r}
ordinalRegressionModel <- robcov(orm(orderedVariable ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                     data = mydata,
                                     x = TRUE,
                                     y = TRUE))

ordinalRegressionModel
confint(ordinalRegressionModel)
```

## Bayesian ordinal regression model

```{r}
bayesianOrdinalRegression <- brm(orderedVariable ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                 data = mydata,
                                 family = cumulative(),
                                 chains = 4,
                                 seed = 52242,
                                 iter = 2000)

summary(bayesianOrdinalRegression)
```

## Bayesian count regression model

```{r}
bayesianCountRegression <- brm(countVariable ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                               data = mydata,
                               family = "poisson",
                               chains = 4,
                               seed = 52242,
                               iter = 2000)

summary(bayesianCountRegression)
```

## Logistic regression model (rms)

```{r}
logisticRegressionModel <- robcov(lrm(female ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                      data = mydata,
                                      x = TRUE,
                                      y = TRUE))

logisticRegressionModel
confint(logisticRegressionModel)
```

## Bayesian logistic regression model

```{r}
bayesianLogisticRegression <- brm(female ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                  data = mydata,
                                  family = bernoulli,
                                  chains = 4,
                                  seed = 52242,
                                  iter = 2000)
                                       
summary(bayesianLogisticRegression)
```

# Hierarchical Linear Regression

# Moderated Multiple Regression

# Model Building Steps

1. Examine extent and type of missing data, consider how to handle missing values (multiple imputation, FIML, pairwise deletion, listwise deletion)
    - Little's MCAR test from `mcar_test()` function of the `njtierney/naniar` package
    - Bayesian handling of missing data: https://cran.r-project.org/web/packages/brms/vignettes/brms_missings.html
1. Examine descriptive statistics, consider variable transformations
1. Examine scatterplot matrix to examine whether associations between predictor and outcomes are linear or nonlinear
1. Model building: theory and empiricism (stepwise regression, likelihood ratio tests, k-fold cross validation to check for over-fitting compared to simpler model)
1. Test assumptions
    - Examine whether predictors have linear association with outcome (Residual Plots, Marginal Model Plots, Added-Variable Plots—check for non-horizontal lines)
    - Examine whether residuals have constant variance across levels of outcome and predictors Residual Plots, Spread-Level Plots—check for fan-shaped plot or other increasing/decreasing structure)
    - Examine whether predictors show multicollinearity (VIF)
    - Examine whether residuals are normally distributed (QQ plot and density plot)
    - Examine influence of influential observations (high outlyingness and leverage) on parameter stimates by iteratively removing influential observations and refitting (Added-Variable Plots)
1. Handle violated assumptions, select final set of predictors/outcomes and transformation of each
1. Use k-fold cross validation to identify the best estimation procedure (OLS, MM, or LTS) on the final model
1. Use identified estimation procedure to fit final model and determine the best parameter point estimates
1. Calculate bootstrapped estimates to determine the confidence intervals around the parameter estimates of the final model

# Diagnostics

## Assumptions

### 1. Linear relation between predictors and outcome

#### Ways to Test

##### Before Model Fitting

- scatterplot matrix

##### After Model Fitting

Check for nonlinearities (non-horizontal line) in plots of:
- Residuals versus fitted values (Residual Plots) --- best
- Residuals versus predictors (Residual Plots)
- Outcome versus fitted values (Marginal Model Plots)
- Outcome versus predictors, ignoring other predictors (Marginal Model Plots)
- Outcome versus predictors, controlling for other predictors (Added-Variable Plots)

#### Ways to Handle

- Transform outcome/predictor variables (Box-Cox transformations)
- Semi-parametric regression models: Generalized additive models (GAM)
- Non-parametric regression models: Nearest-Neighbor Kernel Regression

### 2. Exogeneity

Exogeneity means that the association between predictors and outcome is fully causal and unrelated to other variables.

#### Ways to Test

- Durbin-Wu-Hausman test of endogeneity

#### Ways to Handle

- Conduct an experiment/RCT with random assignment
- Instrumental variables

### 3. Homoscedasticity of residuals

Homoscedasticity of the residuals means that the variance of the residuals does not differ as a function of the outcome/predictors (i.e., the residuals show constant variance as a function of outcome/predictors).

#### Ways to Test

- Plot residuals vs. outcome and predictor variables (Residual Plots)
- Plot residuals vs. fitted values (Residual Plots)
- Time Series data: Plot residuals vs. time
- Spread-level plot
- Breusch-Pagan test: `bptest()` function from `lmtest` package
- Goldfeld-Quandt Test

#### Ways to Handle

- If residual variance increases as a function of the fitted values, consider a poisson regression model (especially for count data), for which the variance does increase with the mean
- If residual variance differs as a function of model predictors, consider adding interactions/product terms (the effect of one variable depends on the level of another variable)
- Try transforming the outcome variable to be normally distributed (log-transform if the errors seem consistent in percentage rather than absolute terms)
- If the error variance is proportional to a variable z, then fit the model using Weighted Least Squares (WLS), with the weights given be 1/z
- Weighted least squares (WLS) using the "weights" argument of the `lm()` function
- Huber-White standard errors (a.k.a. "Sandwich" estimates) from a heteroscedasticity-corrected covariance matrix
    - `coeftest()` function from the `sandwich` package along with hccm sandwich estimates from the `car` package 
    - `robcov()` function from the `rms` package
- Time series data: ARCH (auto-regressive conditional heteroscedasticity) models
- Time series data: seasonal patterns can be addressed by applying log transformation to outcome variable

### 4. Errors are independent

Independent errors means that the errors are uncorrelated with each other.

#### Ways to Test

- Plot residuals vs. predictors (Residual Plots)
- Time Series data: Residual time series plot (residuals vs. row number)
- Time Series data: Table or plot of residual autocorrelations
- Time Series data: Durbin-Watson statistic for test of significant residual autocorrelation at lag 1

#### Ways to Handle

- Generalized least squares (GLS) models are capable of handling correlated errors
- Regression with cluster variable
    - `robcov()` from `rms` package
- Multilevel modeling
    - Linear mixed effects models
    - Generalized linear mixed effects models
    - Nonlinear mixed effects models

### 5. No multicollinearity

Multicollinearity occurs when the predictors are correlated with each other.

#### Ways to Test

- Variance Inflation Factor (VIF)
- Generalized Variance Inflation Factor (GVIF)—when models have related regressors (multiple polynomial terms or contrasts from same predictor)
- Correlation
- Tolerance
- Condition Index

#### Ways to Handle

- Remove highly correlated (i.e., redundant) predictors
- Average the correlated predictors
- Principal Component Analysis for data reduction
- Standardize predictors
- Center the data (deduct the mean)
- Singular-value decomposition of the model matrix or the mean-centered model matrix
- Conduct a factor analysis and rotate the factors to insure independence of the factors

### 6. Errors are normally distributed

#### Ways to Test

- Probability Plots
    - Normal Quantile (QQ) Plots (based on non-cumulative distribution of residuals)
    - Normal Probability (PP) Plots (based on cumulative distribution of residuals)
- Density Plot of Residuals
- Statistical Tests
    - Kolmogorov-Smirnov test
    - Shapiro-Wilk test
    - Jarque-Bera test
    - Anderson-Darling test (best test)
- Examine influence of outliers

#### Ways to Handle

- Apply a transformation to the predictor or outcome variable
- Exclude outliers
- Robust regression
    - Best when no outliers: MM-type regression estimator
        - `lmrob()`/`glmrob()` function of `robustbase` package
        - Iteratively reweighted least squares (IRLS): `rlm(, method = "MM")` function of `MASS` package: http://www.ats.ucla.edu/stat/r/dae/rreg.htm
    - Most resistant to outliers: Least trimmed squares (LTS)—but not good as a standalone estimator, better for identifying outliers
        - `ltsReg()` function of `robustbase` package (best)
    - Best when single predictor: Theil-Sen estimator
        - `mblm(, repeated = FALSE)` function of `mblm` package
    - Robust correlation
        - Spearman's rho: `cor(, method = "spearman")`
        - Percentage bend correlation
        - Minimum vollume ellipsoid
        - Minimum covariance determinant:
        - Winsorized correlation
        - Biweight midcorrelation
    - Not great options:
        - Quantile (L1) regression: `rq()` function of `quantreg` package

### Resources

Book: An R Companion to Applied Regression

http://people.duke.edu/~rnau/testing.htm

http://www.statmethods.net/stats/rdiagnostics.html

http://socserv.socsci.mcmaster.ca/jfox/Courses/Brazil-2009/index.html

https://en.wikipedia.org/wiki/Ordinary_least_squares#Assumptions

# Session Info

```{r}
sessionInfo()
```
